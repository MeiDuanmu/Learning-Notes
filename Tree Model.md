## Decision Tree <br>
* entropy
* gini index vs information gain
* ID3, C4.5, CART, CHAID
* Hypoparameter (max depth, min samples in a node, min samples to split, max leaf nodes)

## Random Forecast <br>
* ensemble trees (bagging method)
* Hyperparameters (number of features, number of trees)
* Advantages (accuracy, robust, handle missing/outlier, feature importance, easier to train)


## GBN <br>
* ensamble trees (boosting method)
* Hyperparameters (learning rate, numbers of trees, tree depth)
* Advantages (accuracy, robust, handle missing/outlier, feature importance)

